{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import gensim.models.word2vec as w2v \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE \t\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open(\"Span-Eng.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    sentences = [line.strip(\"\\n\").split(\"|\") for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['oh', 'eng'],\n",
       " [',', 'punct'],\n",
       " ['no', 'eng'],\n",
       " ['.', 'punct'],\n",
       " ['i', 'eng'],\n",
       " ['am', 'eng'],\n",
       " ['not', 'eng'],\n",
       " ['gonna', 'eng'],\n",
       " ['take', 'eng'],\n",
       " ['her', 'eng']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "i = 0\n",
    "while i in range(len(sentences)): \n",
    "    x.append(sentences[i][0])\n",
    "    y.append(sentences[i][1])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  corpus contains 25,963 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(line) for line in x])\n",
    "print(\"The  corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings = w2v.Word2Vec(x, min_count=0, size=300, window=1, workers=15, sample=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [x.split() for x in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings = w2v.Word2Vec(word_list, min_count=2, size=300, window=1, workers=15, sample=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings vocabulary length: 604\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings vocabulary length:\", len(Embeddings.wv.vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Embeddings.save(\"Spanglish.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Embeddings[Embeddings.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x118cafda0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyplot.scatter(result[:, 0], result[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = list(Embeddings.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAERCAYAAACtswpGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a073780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, io\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import keras.utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Dense, LSTM, Dropout, Activation, TimeDistributed, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "from pyexcel_xlsx import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH=100\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "documents = []\n",
    "labels = []\n",
    "labels_index = {}\n",
    "ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'podemos'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'span'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n"
     ]
    }
   ],
   "source": [
    "print(\"tokenizing\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(x)#texts\n",
    "sequences = tokenizer.texts_to_sequences(x)#texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pre-trained word embeddings \n",
    "gensimemb = Word2Vec.load('Spanglish.w2v')\n",
    "embedding_matrix = np.zeros((len(gensimemb.wv.vocab), EMBEDDING_DIM))\n",
    "for i in range(len(gensimemb.wv.vocab)):\n",
    "    embvec = gensimemb.wv[gensimemb.wv.index2word[i]]\n",
    "    if embvec is not None:\n",
    "        embedding_matrix[i] = embvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1287 unique tokens.\n",
      "padding\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('found %s unique tokens.' % len(word_index))\n",
    "print(\"padding\")\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "print('Encoding labels...')\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(y)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "labels = integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "labels = keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shuffled and split\n"
     ]
    }
   ],
   "source": [
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "print('Data shuffled and split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(604, 300, embeddings_initializer=\"glorot_uniform\", mask_zero=True, trainable=True, input_length=100, weights=[array([[ ..., embeddings_regularizer=None)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_9 to have shape (None, 5) but got array with shape (5608, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-eb6b96daf860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1383\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1384\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_9 to have shape (None, 5) but got array with shape (5608, 4)"
     ]
    }
   ],
   "source": [
    "weight_decay = 0\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import *\n",
    "from keras.regularizers import l2\n",
    "regularizer = l2(weight_decay) if weight_decay else None\n",
    "\n",
    "TOTAL_WORDS, EMBEDDING_DIM = embedding_matrix.shape\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCHSIZE = 128\n",
    "OUTPUTDIM = 5\n",
    "#optimizer\n",
    "adam = optimizers.Adam(lr=0.9, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "print (\"Building Model...\")\n",
    "embedding_layer = Embedding(TOTAL_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], mask_zero=True,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,W_regularizer=regularizer,\n",
    "                            trainable=True, embeddings_initializer=\"glorot_uniform\")\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = LSTM(128,activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform',\n",
    "         recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True )(embedded_sequences) # <- 128 = num_hidden_nodes\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('sigmoid')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = LSTM(64)(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(OUTPUTDIM, activation='softmax')(x) #hyperas\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print (\"Training...\")\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCHSIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-103e37a3d014>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Converts the labels to a one-hot representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Converts the labels to a one-hot representation\n",
    "import numpy as np\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5607\n",
      "Test size: 2404\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "train_size = int(len(x) * .7)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(x) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain = data[:train_size]\n",
    "xtest = data[train_size:]\n",
    "ytrain = y_new[:train_size]\n",
    "ytest = y_new[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Embedding` call to the Keras 2 API: `Embedding(604, 300, embeddings_initializer=\"glorot_uniform\", mask_zero=True, trainable=True, input_length=100, weights=[array([[ ..., embeddings_regularizer=None)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (None, 5) but got array with shape (5607, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6ac37a20a36a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCHSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1380\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1383\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1384\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (None, 5) but got array with shape (5607, 1)"
     ]
    }
   ],
   "source": [
    "weight_decay = 0\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import *\n",
    "from keras.regularizers import l2\n",
    "regularizer = l2(weight_decay) if weight_decay else None\n",
    "\n",
    "TOTAL_WORDS, EMBEDDING_DIM = embedding_matrix.shape\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCHSIZE = 128\n",
    "OUTPUTDIM = 5\n",
    "#optimizer\n",
    "adam = optimizers.Adam(lr=0.9, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-6)\n",
    "print (\"Building Model...\")\n",
    "embedding_layer = Embedding(TOTAL_WORDS,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix], mask_zero=True,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,W_regularizer=regularizer,\n",
    "                            trainable=True, embeddings_initializer=\"glorot_uniform\")\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = LSTM(128,activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform',\n",
    "         recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True )(embedded_sequences) # <- 128 = num_hidden_nodes\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('sigmoid')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = LSTM(64)(embedded_sequences)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(OUTPUTDIM, activation='softmax')(x) #hyperas\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "print (\"Training...\")\n",
    "model.fit(xtrain, ytrain, validation_split=0.2, epochs=EPOCHS, batch_size=BATCHSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
