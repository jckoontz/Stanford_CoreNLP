{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    " \n",
    "import json\n",
    " \n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "with io.open(\"spanglish_lt4.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    spline = [line.strip('\\n').split(\";\") for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "texts = []\n",
    "labels = []\n",
    "for i in range(len(spline)): \n",
    "    texts.append(spline[i][0])\n",
    "    labels.append(spline[i][1])\n",
    "\n",
    "punct = {\"!\", \"?\", \".\", \"-\",\"UNK\", \",\"}\n",
    "i = 0\n",
    "#print(texts[i] in punct)\n",
    "while(i in range(len(texts))): \n",
    "    if(texts[i] == \"<unk>\"): \n",
    "        labels[i] = \"<unk>\"\n",
    "    if(texts[i]) in punct:\n",
    "        labels[i] = \"punct\"\n",
    "        #print(\"TRUE\") \n",
    "    #else: \n",
    "        #print(\"FALSE\")\n",
    "        #print(labels[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', ',', 'no', '.', 'i', 'am', 'not', 'gonna', 'take', 'her']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]\n",
    "#or text in texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'complication': 673, 'graduated': 793, 'vi': 736, 'nosotros': 845, 'well': 32, 'ago': 332, 'pract': 959, 'mijo': 1254, 'american': 470, 'recuérdame': 918, 'became': 1167, 'all': 59, 'mean': 111, \"o'clock\": 378, 'medical': 653, 'cup': 456, 'work': 534, 'piden': 687, 'fifteen': 250, 'mi': 1181, 'whoa': 939, 'opened': 743, 'salirse': 1222, 'his': 176, 'alex': 428, 'porque': 137, 'place': 753, 'intensive': 650, 'twelve': 427, 'tailgate': 963, 'though': 287, 'recordó': 806, 'um': 150, 'when': 84, 'dije': 23, 'graduate': 202, 'resident': 549, 'upset': 313, 'bet': 914, 'chicken': 490, 'mes': 649, 'sounds': 438, 'untarle': 1275, 'sentaron': 423, 'into': 1033, 'sliding': 591, 'agarro': 1257, 'okay': 630, 'elote': 797, 'surprised': 1061, 'girl': 233, 'matter': 1113, 'wanna': 188, 'pachanga': 971, 'central': 788, 'email': 919, 'misa': 642, 'even': 180, 'michael': 328, 'thousand': 219, 'huerta': 781, 'try': 1176, 'las': 103, 'as': 503, 'donald': 566, 'haz': 348, 'chico': 1269, 'quién': 857, 'trying': 752, 'fin': 596, 'mismo': 1126, 'nuh': 547, 'something': 101, 'group': 983, 'passed': 454, 'dollar': 1170, 'haber': 1045, 'hill': 777, 'torre': 879, 'vaya': 397, 'campus': 272, 'ayudar': 1237, 'faxed': 1035, 'uh': 33, 'book': 309, \"ta's\": 1177, 'parade': 417, 'muy': 120, 'lap': 1158, 'ya': 56, 'caja': 614, 'accompleme': 883, 'number': 1076, 'más': 174, 'logging': 535, 'one': 45, 'sat': 540, 'marisa': 702, 'que': 26, 'cuen': 1173, 'change': 376, 'microwave': 497, 'completo': 402, 'remark': 864, 'nhh': 451, 'causar': 1046, 'area': 330, 'soy': 421, 'grande': 359, 'send': 114, 'creció': 575, 'ah': 294, 'tonto': 1224, 'gusta': 161, 'byron': 990, 'love': 379, 'ten': 382, 'hear': 351, 'guy': 448, 'it': 10, 'accidents': 1085, 'preocupando': 938, 'grandes': 763, 'possibly': 1017, 'quince': 912, 'while': 531, 'quedó': 386, 'eché': 1252, 'leo': 776, 'thought': 191, 'that': 8, 'algo': 1276, 'cemetery': 639, 'otros': 1074, 'razón': 683, 'laptop': 1155, 'mano': 413, 'hacía': 1240, 'upstairs': 270, 'borrow': 1095, 'llegó': 522, 'still': 158, 'nah': 1010, 'sí': 76, 'shows': 895, 'any': 926, 'esposa': 824, 'hand': 523, 'talking': 391, 'mueve': 1204, 'todas': 1241, 'h': 690, 'back': 236, 'tienes': 678, 'an': 163, 'beans': 944, 'care': 369, 'mmmhmm': 621, 'lola': 357, 'police': 240, 'familiar': 782, 'making': 1063, 'sense': 505, 'waiting': 529, 'become': 1175, 'digo': 49, 'trailers': 345, 'everybody': 982, 'expensive': 518, 'happen': 281, 'salas': 691, 'person': 839, 'knows': 958, 'problem': 1111, 'ir': 384, 'sure': 130, 'por': 110, 'pararon': 890, 'university': 1012, 'martin': 849, 'mejor': 817, 'tú': 468, 'did': 38, 'metió': 586, 'último': 734, 'buys': 1082, 'please': 436, 'crying': 1171, 'vez': 324, 'deal': 779, 'finally': 337, 'aquí': 368, 'con': 173, 'supposed': 569, 'eras': 1141, 'podemos': 366, 'stupid': 1282, 'both': 298, 'dad': 455, 'outside': 280, 'si': 89, 'hace': 170, 'paying': 843, 'make': 102, 'embajada': 681, 'because': 47, 'husband': 464, 'which': 465, 'recojo': 612, 'ready': 1114, 'gets': 1198, 'hard': 951, 'force': 437, 'tw': 1001, 'those': 208, 'looking': 521, 'perro': 165, 'exactly': 509, 'wait': 463, 'open': 1229, 'tierna': 1212, 'grandota': 1208, 'play': 370, 'amount': 1055, 'nueve': 664, 'show': 469, 'meticulous': 1062, 'goes': 854, 'cute': 573, 'thumbs': 775, 'tienen': 932, 'appoint': 514, 'nunca': 1028, 'unusual': 853, 'long': 559, 'babies': 1285, 'our': 312, 'thinking': 199, 'está': 230, 'minutes': 481, 'tontolon': 1225, 'more': 200, 'on': 74, 'saber': 925, 'register': 100, 'también': 135, 'figure': 930, 'eitale': 795, 'dirección': 1130, 'oof': 730, 'diligent': 931, 'stuff': 1053, 'includes': 1002, 'use': 561, 'cuando': 186, 'supplies': 1183, 'ummm': 998, 'taller': 830, 'primera': 1215, 'nombre': 401, 'nine': 431, 'want': 117, 'computer': 536, 'antibiotics': 407, 'kai': 1102, 'tu': 282, 'apenitas': 746, 'consciente': 738, 'student': 269, 'sofa': 1280, 'ok': 129, 'apartment': 221, 'true': 1154, 'puppy': 356, 'stand': 314, 'him': 44, 'cafecita': 598, 'carlos': 327, 'dar': 473, 'podía': 1236, 'contracts': 1041, 'start': 498, 'vio': 1232, 'look': 429, 'hablar': 367, 'turning': 707, 'gave': 486, 'cabo': 697, 'your': 94, 'vamos': 700, 'two': 72, 'awake': 739, 'type': 555, 'atrás': 1244, 'de': 34, 'pay': 126, 'supe': 668, 'was': 28, 'directly': 1138, 'worse': 1213, 'sábado': 662, 'raspberry': 633, 'second': 755, 'driver': 354, 'benefits': 822, 'hours': 510, 'chinitas': 703, 'diga': 711, 'diablo': 1107, 'often': 418, 'india': 513, 'pictures': 1211, 'time': 123, 'michelle': 783, 'ojo': 414, 'punta': 1106, 'tamañote': 1270, 'bueno': 352, 'hnnn': 803, 'tí': 1124, 'text': 511, 'rosy': 1115, 'va': 83, 'ta': 640, 'this': 86, 'seventeen': 766, 'late': 771, 'came': 917, 'whole': 405, 'rate': 1168, 'case': 617, 'agarré': 303, 'frog': 658, 'sixteen': 770, 'cállate': 704, 'heavy': 1251, 'yendo': 1186, 'agua': 1205, 'family': 643, 'desde': 680, 'under': 563, 'salida': 852, 'went': 124, 'registering': 344, 'wrong': 1153, 'away': 317, 'ooh': 247, 'good': 315, 'como': 106, 'lunching': 913, 'medicina': 760, 'enoja': 1145, 'close': 319, 'paid': 217, 'oh': 39, 'ed': 476, 'creciendo': 599, 'once': 1058, 'arteries': 731, 'reception': 525, 'poor': 976, 'al': 88, 'between': 769, 'loosed': 677, 'rufus': 1223, 'ratito': 1072, 'poem': 808, 'rachel': 560, 'cambio': 1174, 'floors': 1090, 'years': 507, 'have': 22, 'alive': 838, 'entraba': 1234, 'expenses': 389, 'marrying': 827, 'cansa': 277, 'us': 506, 'trailer': 80, 'caí': 1281, 'served': 924, 'anyway': 891, 'dio': 404, 'watch': 374, 'many': 489, 'she': 42, 'showing': 921, 'take': 107, 'signatures': 1036, 'science': 537, 'each': 475, 'duró': 646, 'eighteen': 767, 'pass': 434, 'otra': 1258, 'tall': 828, 'trouble': 260, 'house': 372, 'boyfriends': 661, 'of': 31, 'too': 196, 'encanta': 205, 'cómo': 190, 'going': 175, 'slurped': 909, 'same': 122, 'watching': 371, 'wow': 243, 'mattered': 905, 'afternoon': 850, 'amuy': 745, 'speechless': 977, 'real': 949, 'faculty': 915, 'sale': 1192, 'commander': 873, 'ven': 1189, 'dando': 721, 'toma': 1255, 'tanta': 802, 'seventy': 792, 'wars': 420, 'nervous': 676, 'holiday': 1015, 'cuánto': 645, 'blue': 674, 'los': 321, 'juguetona': 1185, 've': 722, 'desk': 541, 'bye': 299, 'dijeron': 242, 'sit': 619, 'cannot': 177, 'grassy': 962, 'una': 152, 'carita': 1209, 'friends': 416, 'daniel': 304, 'zacate': 965, 'respect': 904, 'others': 1044, 'levantó': 1231, 'pego': 671, 'copies': 1064, 'corriendo': 1193, 'pagaron': 934, 'llego': 896, 'buy': 546, 'what': 29, 'study': 1050, 'todavia': 906, 'mary': 353, 'handy': 1014, 'acá': 554, 'carol': 969, 'esto': 504, 'saying': 1099, 'beautiful': 813, 'vacation': 693, 'lo': 67, 'complicaciones': 667, 'lado': 594, 'dog': 587, 'nothing': 323, 'officers': 652, 'results': 987, 'carácter': 441, 'come': 517, 'yesterday': 1080, 'brings': 1088, 'brincó': 1277, 'israel': 1156, 'goodbye': 601, 'sold': 1077, 'overwhelming': 1054, 'difícil': 558, 'niña': 1190, 'up': 82, 'decir': 701, 'heard': 1023, 'think': 92, 'quede': 611, 'said': 75, 'andar': 856, 'metiendo': 1226, 'pesado': 874, 'treated': 836, 'hacerse': 1120, 'hospital': 647, 'plan': 920, 'steak': 940, 'dijo': 46, 'complicada': 1121, 'increase': 567, 'robles': 692, 'voy': 1128, 'trips': 1147, 'finished': 957, 'son': 415, 'toys': 622, 'sad': 290, 'semillas': 800, 'revés': 1243, 'last': 453, 'office': 115, 'carmel': 638, 'agarrada': 876, 'respetos': 862, 'attention': 1194, 'texas': 226, 'soup': 631, 'questions': 335, 'before': 528, 'qué': 132, 'fare': 1019, 'amigos': 751, 'agacharme': 1274, 'abajo': 733, 'oye': 1283, 'he': 5, 'new': 183, 'face': 754, 'home': 629, 'potato': 493, 'related': 520, 'funny': 715, 'students': 216, 'red': 708, 'moment': 899, 'mm': 757, 'line': 1148, 'potatoes': 495, 'information': 564, 'inn': 1016, 'who': 118, 'blame': 978, 'lot': 246, 'sea': 400, 'bañe': 1217, 'steve': 936, 'seven': 1009, 'casserole': 947, 'thinks': 1273, 'brown': 364, 'leash': 435, 'mandan': 533, 'jala': 579, 'mix': 946, 'either': 267, 'roger': 228, 'arguing': 881, 'then': 52, 'services': 1021, 'whatever': 148, 'doo': 589, 'after': 527, 'salad': 943, 'martha': 585, 'forgave': 900, 'loves': 358, 'daba': 1235, 'call': 342, 'tuve': 1260, 'bigger': 1207, 'know': 16, 'lunch': 1287, 'chairs': 500, 'department': 543, 'corn': 796, 'caer': 602, 'rose': 741, 'jugando': 1263, 'dijiste': 1140, 'signature': 1039, '1': 872, 'dónde': 383, 'en': 57, 'dices': 871, 'agarró': 307, 'not': 17, 'concerned': 960, 'estamos': 698, 'longer': 818, 'spread': 718, 'arrimé': 740, 'tomato': 615, 'green': 508, 'virus': 153, 'list': 572, 'abrió': 305, 'caro': 723, 'gracias': 654, 'piquiniqui': 970, 'provost': 143, 'four': 214, 'unless': 1079, 'makes': 985, 'other': 157, 'quiere': 627, 'guys': 488, 'ay': 556, 'carried': 524, 'toti': 227, 'easy': 373, 'how': 77, 'le': 14, 'enloda': 582, 'van': 300, 'lawn': 966, 'teh': 772, 'notary': 349, 'apaga': 1286, 'used': 785, 'cinco': 628, 'wanted': 494, 'un': 136, 'shower': 279, 'umm': 108, 'huh': 98, 'hizo': 408, 'se': 37, 'single': 565, 'stop': 474, 'hija': 805, 'remind': 935, 'personal': 973, 'cop': 253, 'there': 50, 'are': 30, 'movía': 1245, 'documentación': 686, 'connections': 847, 'sent': 159, 'tenía': 394, 'changos': 972, 'yo': 116, 'mention': 794, 'every': 259, 'refrigerator': 635, 'uno': 764, 'anybody': 1101, 'quería': 1221, 'sign': 213, 'wake': 457, 'toda': 759, 'resbalaba': 1248, 'sacarla': 1219, 'fifty': 1169, 'own': 1057, 'in': 40, 'man': 239, 'jenny': 1163, 'almost': 888, 'cola': 1203, 'ri': 606, 'nomás': 187, 'haces': 696, 'save': 980, 'notarized': 1098, 'star': 419, 'tape': 974, 'or': 58, 'dragon': 846, 'voice': 657, 'a': 7, 'ninguna': 603, 'approved': 532, 'grupo': 844, 'badal': 993, 'id': 346, 'kept': 1034, 'around': 1069, 'hundred': 220, 'nomas': 689, 'quick': 950, 'este': 395, 'week': 296, 'evaluation': 331, 'ahí': 146, 'entonces': 716, 'forty': 430, 'my': 258, 'rather': 869, 'phone': 1166, 'parts': 1081, 'pepper': 1182, 'practiced': 499, 'called': 128, 'profit': 1093, 'aurelio': 306, 'uhh': 1097, 'we': 53, 'old': 768, 'name': 254, 'copy': 223, 'volunteer': 1146, 'stack': 1083, 'helping': 995, 'ser': 967, 'down': 252, 'only': 178, 'ericka': 1029, 'aww': 997, 'power': 574, 'dejamos': 928, 'fi': 1196, 'end': 826, 'everyday': 996, 'were': 54, 'pero': 62, 'darte': 1122, 'mailed': 1026, 'ahora': 577, 'light': 1268, 'with': 60, 'go': 81, 'simpática': 823, 'deliveries': 1056, 'comer': 933, 'big': 181, 'steal': 480, 'never': 255, 'anything': 184, 'puts': 1089, 'erraron': 1065, 'at': 61, 'reminder': 482, 'residente': 1142, 'wants': 215, 'asking': 1144, 'weak': 732, 'endrogada': 644, 'chillona': 422, 'could': 229, 'complicated': 1112, 'somewhere': 249, 'tantos': 819, 'chavita': 446, 'knew': 784, 'mí': 131, 'looked': 411, 'antes': 710, 'reverse': 655, 'insurance': 388, 'mandé': 684, 'celaba': 1197, 'lawsuit': 660, 'manos': 362, 'butter': 948, 'joven': 666, 'missing': 810, 'test': 334, 'hijole': 478, 'te': 134, 'chaff': 616, 'quality': 842, 'diaper': 625, 'audio': 1024, 'terminen': 608, 'plus': 519, 'need': 235, 'five': 445, 'pagan': 929, 'miss': 814, 'yeah': 19, 'fighting': 885, 'cook': 952, 'learned': 1150, 'fall': 1179, 'so': 20, 'tiempo': 893, 'trompa': 1278, 'ex': 790, 'pobre': 460, 'di': 597, 'trabaja': 820, 'provided': 1022, 'took': 302, 'thing': 113, 'imagine': 1018, 'laughing': 1152, 'vas': 467, 'always': 256, 'adrian': 447, 'frente': 424, 'lisa': 204, 'legion': 471, 'baked': 945, 'cheapest': 1008, 'hagan': 1007, 'doubt': 825, 'ver': 593, 'money': 292, 'así': 66, 'oscuro': 1265, 'bañarla': 583, 'early': 432, 'funerals': 466, 'should': 392, 'megadoses': 726, 'now': 155, 'esta': 140, 'way': 209, 'himself': 620, 'macs': 1159, 'maneras': 1242, 'kid': 618, 'yet': 1178, 'conference': 694, 'coffee': 318, 'first': 138, 'carta': 552, 'frank': 1157, 'whammy': 1135, 'put': 241, 'telling': 1127, 'nada': 322, 'él': 308, 'verdad': 172, 'cansado': 1249, 'ques': 986, 'gone': 840, 'clashed': 750, 'intenté': 1216, 'much': 212, 'understand': 1051, \"twenty's\": 774, 'says': 225, 'tiene': 361, 'me': 21, 'minute': 483, 'allá': 207, 'bien': 231, 'parecía': 1246, 'tables': 501, 'august': 1180, 'really': 154, 'acquainted': 834, 'estaba': 198, 'staying': 1004, 'to': 11, 'visa': 398, 'wife': 1151, 'hall': 539, 'artery': 729, 'see': 104, 'broken': 316, 'president': 182, 'andaba': 472, 'had': 64, 'out': 197, 'toalla': 1253, 'mira': 1272, 'key': 1096, 'voices': 656, 'pongo': 613, 'dollars': 516, 'hour': 289, 'vuelta': 592, 'fluids': 762, 'strange': 798, 'país': 688, 'hinchado': 758, 'vieras': 855, 'houston': 685, 'unk': 12, 'vida': 557, 'cut': 496, 'tina': 265, 'wonder': 461, 'weeks': 224, 'another': 169, 'eight': 1000, 'parsa': 992, 'fui': 848, 'no': 9, 'yelling': 886, 'whether': 1037, 'vacaciones': 695, 'nos': 902, 'primero': 485, 'tan': 276, 'querer': 1116, 'about': 79, 'three': 211, 'appointed': 1160, 'circo': 1214, 'tell': 127, 'grad': 568, 'again': 458, 'sixty': 1094, 'doing': 262, 'confusión': 1047, 'eye': 744, 'collection': 385, 'retired': 778, 'complica': 1118, 'retention': 761, 'enfrente': 425, 'tells': 1164, 'piñones': 450, 'patitas': 584, 'days': 218, 'the': 1, 'doors': 1228, 'remember': 325, 'el': 51, 'original': 343, 'whoops': 705, 'mañana': 910, 'bonito': 440, 'they': 13, 'some': 147, 'china': 244, 'wha': 1078, 'tere': 491, 'luego': 68, 'getting': 942, 'will': 71, 'lejos': 641, 'through': 195, 'door': 1230, 'later': 1066, 'baby': 232, 'transfer': 1133, 'meet': 1108, 'nuevo': 551, 'risadas': 714, 'near': 1011, 'cake': 954, 'young': 251, 'reason': 526, 'such': 865, 'mama': 894, 'entire': 720, 'mal': 724, 'done': 234, 'birthday': 955, 'quedará': 867, 'over': 105, 'ahem': 988, 'sheridan': 1005, 'prepayed': 484, 'mont': 637, 'bury': 636, 'andaron': 1162, 'little': 109, 'been': 133, 'ella': 203, 'tipo': 669, 'gusto': 858, 'protective': 1199, 'right': 168, 'ones': 1060, 'attacking': 719, 'female': 1188, 'lucas': 968, 'enojamos': 903, 'hmm': 25, 'does': 91, 'lost': 222, 'air': 339, 'hotel': 1013, 'esos': 1227, 'highway': 1075, 'cooking': 956, 'peleonero': 875, 'for': 43, 'cosas': 1073, 'veras': 1104, 'iban': 878, 'next': 403, \"survivor's\": 821, 'their': 142, 'muffins': 634, 'them': 63, 'grandma': 600, 'corazon': 672, 'oreja': 1202, 'paró': 807, 'forgot': 477, 'parecían': 799, 'todo': 275, 'accomplish': 884, 'creas': 1271, 'than': 201, 'accept': 1038, 'bonita': 1210, 'abandoned': 1086, 'eahh': 1187, 'told': 144, 'para': 87, 'hasta': 350, 'afraid': 365, 'orsp': 341, 'volteaba': 363, 'deana': 937, 'lío': 1220, 'tumbó': 1279, 'muertito': 866, 'es': 69, 'hacer': 1125, 'kno': 626, 'why': 95, 'car': 347, 'find': 336, 'can': 97, 'leave': 286, 'forget': 1040, 'mas': 1266, 'today': 381, 'últimos': 861, 'bring': 329, 'traer': 492, 'entrada': 851, 'found': 727, 'interfere': 880, 'i': 2, 'registered': 544, 'talks': 907, 'hey': 449, 'están': 283, 'somebody': 193, 'payment': 1043, 'year': 444, 'you': 4, 'sides': 941, 'noodles': 632, 'problemas': 1123, 'food': 167, 'gold': 981, 'recording': 975, 'cree': 1262, 'paso': 1143, 'mexico': 164, 'things': 320, 'escapó': 1259, 'eventually': 1067, 'hice': 571, 'años': 665, 'sarah': 898, 'be': 73, 'am': 35, 'by': 149, 'owned': 1100, 'twenty': 125, 'commission': 548, 'sitting': 271, 'lives': 1105, 'belong': 1132, 'principio': 310, 'nice': 502, 'ridiculous': 882, 'entrar': 377, 'secala': 1256, 'letting': 747, 'get': 78, 'left': 375, 'casi': 648, 'les': 1139, 'limón': 801, 'teens': 773, 'kids': 311, 'taking': 284, 'difference': 512, 'tomatoes': 380, 'scooby': 278, 'these': 338, 'together': 1059, 'la': 24, 'fue': 410, 'link': 811, 'dieron': 725, 'bought': 545, 'least': 1020, 'waking': 816, 'muchas': 804, 'passing': 833, 'pensaba': 877, 'double': 1134, 'hmmm': 624, 'parece': 550, 'but': 41, 'holding': 832, 'rush': 1032, 'teníamos': 911, 'mmm': 36, 'venir': 553, 'puras': 713, 'tomorrow': 266, 'bully': 791, 'day': 263, 'obituary': 812, 'assuming': 789, 'costs': 999, 'saw': 443, 'policeman': 651, 'like': 55, 's': 991, 'order': 953, 'gonna': 27, 'café': 787, 'rita': 607, 'helped': 756, 'deste': 1233, 'technical': 989, 'sells': 1092, 'medio': 459, 'del': 261, 'convinced': 1137, 'license': 274, 'pum': 1191, 'worked': 780, 'nobody': 206, 'half': 1149, 'happened': 396, 'brinca': 1201, 'compró': 1109, 'consideration': 922, 'avión': 1247, 'tengo': 406, 'brinque': 581, 'caía': 1238, 'muerde': 580, 'eleven': 248, 'from': 93, 'murió': 393, 'since': 245, 'living': 1003, 'atendió': 717, 'toothpick': 1136, 'bit': 297, 'pésame': 897, 'paints': 1091, 'agarrar': 712, 'bad': 737, 'jump': 355, 'lemme': 675, 'do': 18, 'morning': 237, 'ask': 141, 'agradeció': 908, 'solo': 1119, 'poco': 452, 'letter': 145, 'tech': 487, 'dice': 166, 'tired': 1250, 'nn': 670, 'gosh': 979, 'address': 1131, 'uso': 1129, 'give': 210, 'until': 288, 'people': 96, 'if': 70, 'maybe': 112, 'grabando': 699, 'risa': 1239, 'having': 542, 'personality': 333, 'happens': 916, 'feeling': 387, 'guera': 829, 'twice': 562, 'would': 90, 'check': 194, 'sordo': 868, 'funeral': 238, 'set': 961, 'party': 964, 'era': 439, 'rounds': 1068, 'eh': 291, 'tv': 285, 'age': 433, 'life': 835, 'and': 6, 'anymore': 1110, 'else': 293, 'notice': 841, 'tus': 860, 'forever': 1025, 'barely': 831, 'hello': 515, 'mail': 1049, \"'s\": 99, 'dean': 390, 'has': 189, 'everything': 326, 'body': 301, 'unos': 426, 'ever': 901, 'o': 185, 'note': 1031, 'very': 156, 'approve': 1048, 'hinton': 538, 'undergraduate': 1161, 'is': 3, 'pasó': 892, 'negro': 1264, 'here': 119, 'dark': 1267, 'olvídense': 609, 'días': 1206, 'also': 623, 'her': 65, 'pedirlo': 679, 'corrió': 748, 'research': 268, 'junkyards': 1087, 'estuvo': 1284, 'grandote': 590, 'pagar': 859, 'chihuahua': 682, 'cada': 1071, 'likes': 870, 'rent': 1042, 'leaving': 709, 'must': 192, 'eaten': 923, 'sorry': 1030, 'enough': 887, 'sienta': 1200, 'ponen': 295, 'help': 409, 'oiga': 706, 'cosa': 604, 'just': 48, 'let': 1103, 'ellos': 765, 'blocked': 728, 'chiquita': 578, 'lazy': 815, 'studies': 1052, 'ponía': 595, 'eat': 264, 'encela': 1195, 'agarrarla': 1261, 'ponemos': 605, 'planning': 659, 'six': 340, 'asked': 530, 'say': 160, 'ni': 927, 'grabadora': 610, 'kind': 151, 'y': 15, 'señor': 1117, 'cuenta': 588, 'seen': 462, 'frames': 1084, 'made': 273, 'worry': 479, 'reappoint': 570, 'stopped': 889, 'thirty': 162, 'groups': 984, 'pues': 257, 'pasaporte': 399, 'lágrimas': 749, 'calling': 1165, 'silence': 1184, 'got': 171, 'th': 1172, 'bah': 1027, 'cuarenta': 663, 'mucho': 576, 'patas': 360, 'ponle': 1006, 'quieren': 1070, 'probably': 139, 'dumb': 863, 'already': 179, 'aps': 994, 'spend': 786, 'baquera': 742, 'feliz': 1218, 'easygoing': 442, 'pos': 85, 'noted': 837, 'where': 121, 'pretty': 412, 'estaría': 735, 'chain': 809}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=5)\n",
    "#toytexts = [\"Is is a common word\", \"So is the\", \"the is common\", \"discombobulation is not common\"]\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print(tokenizer.word_index)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "balanced_texts = tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ytrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-23d6c1210ddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mytest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ytrain' is not defined"
     ]
    }
   ],
   "source": [
    "# Use sklearn utility to convert label strings to numbered index\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "print('Encoding labels...')\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "ytrain = encoder.transform(ytrain)\n",
    "ytest = encoder.transform(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_matrix(texts)\n",
    "y = [1,0,0,0,0]\n",
    " \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_dim=vocab_size))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    " \n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    " \n",
    "model.fit(X, y=y, batch_size=200, nb_epoch=700, verbose=0, validation_split=0.2, show_accuracy=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn line two, we add an Embedding layer. \\nThis layer lets the network expand each token to a larger vector,\\nallowing the network to represent words in a meaningful way. \\nWe pass 20000 as the first argument, \\nwhich is the size of our vocabulary (remember, we told the tokenizer to only use the 20 000 most common words earlier), \\nand 128 as the second, which means that each token can be expanded to a vector of size 128. \\nWe give it an input_length of 300, which is the length of each of our sequences.\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=200))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "In line two, we add an Embedding layer. \n",
    "This layer lets the network expand each token to a larger vector,\n",
    "allowing the network to represent words in a meaningful way. \n",
    "We pass 20000 as the first argument, \n",
    "which is the size of our vocabulary (remember, we told the tokenizer to only use the 20 000 most common words earlier), \n",
    "and 128 as the second, which means that each token can be expanded to a vector of size 128. \n",
    "We give it an input_length of 300, which is the length of each of our sequences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4005 samples, validate on 4006 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'even'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-2bd473c08229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'even'"
     ]
    }
   ],
   "source": [
    "model.fit(data, np.array(texts), validation_split=0.5, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
